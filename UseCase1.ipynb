{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00c02de",
   "metadata": {},
   "source": [
    " ## PySpark – Use Case 1 - Resilient Distributed Dataset (RDD)\n",
    "    \n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db6611",
   "metadata": {},
   "source": [
    "### 1. Explain what is RDD and different ways to create it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fa5bb",
   "metadata": {},
   "source": [
    "**RDD: Resilient Distributed Datasets**\n",
    "• Resilient – Fault-tolerant. (Lineage graph)\n",
    "• Distributed – Stored in memory across the cluster on multiple nodes\n",
    "• Dataset – Initial data can come from a file or created programmatically\n",
    "\n",
    "1.RDD is the fundamental data abstraction of Apache Spark.\n",
    "\n",
    "2.RDD represents an immutable, partitioned collection of records that can\n",
    "  be operated on in parallel.\n",
    "  \n",
    "3.Once you create a RDD you cannot change it,but you can create a new RDD by performing operations like Transformation \n",
    "  and Actions on existing RDD.\n",
    "  \n",
    "4.An RDD in spark can be cached and used again for future transformations,which is a huge benefit for users.\n",
    "  RDDs are said to be lazy evaluated.i.e they delay the evaluation until it is really needed.\n",
    "  \n",
    " **There are two different ways to create RDD.**\n",
    "\n",
    " **1.Parallelizing an existing collection in your driver program:-**\n",
    "  \n",
    "  To create an RDD from Paralleized Collections we use the sc.parallelize method where sc stands for spark context, which \n",
    "  can be found under sparksession.\n",
    "  Sparksession Contains: 1. Spark Context\n",
    "                         2. Streaming Context\n",
    "                         3. Sql Context.\n",
    "  So sc.parallelize method is the sparks context parallelized method to create a parallelize collection, which allows\n",
    "  sparks to distribute the data across the multiple nodes.\n",
    "  \n",
    "     my_list = [1, 2, 3, 4, 5]\n",
    "     my_list_rdd = sc.parallelize(my_list)\n",
    "\n",
    "## 2. Referencing to external data file\n",
    "\n",
    "  \n",
    "**2.Referencing a dataset in an external storage system:-**\n",
    " \n",
    "  In Spark, the distributed dataset can be formed from any data source supported by Hadoop, including the local \n",
    "  file system, HDFS, Cassandra, HBase etc.\n",
    "  In this, the data is loaded from the external dataset. To create text file RDD, we can use SparkContext’s textFile method.\n",
    "  It takes URL of the file and read it as a collection of line. URL can be a local path on the machine.\n",
    "  \n",
    "      file_rdd = sc.textFile(\"path_of_file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf464c",
   "metadata": {},
   "source": [
    "### 2. What are features of RDD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8c394",
   "metadata": {},
   "source": [
    "**Features of Spark RDD:-**\n",
    "\n",
    "**1.Immutability-**\n",
    "\n",
    "  You cannot change the state of RDD. If you want to change the state of RDD, you need to create a copy of the existing RDD and   perform your required operations. Hence, the required RDD can be retrieved at any time.\n",
    "\n",
    "**2.In-memory computation-**\n",
    "\n",
    "  Spark supports in-memory computation which stores data in RAM instead of disk. Hence, the computation power of Spark \n",
    "  is highly increased.\n",
    "\n",
    "**3.Lazy evaluation-**\n",
    "\n",
    "  Transformations in RDDs are implemented using lazy operations. In lazy evaluation, the results are not computed immediately.   It will generate the results, only when the action is triggered. Thus, the performance of the program is increased.\n",
    "\n",
    "**4.Fault-tolerant-**\n",
    "\n",
    "  Once you perform any operations in an existing RDD, a new copy of that RDD is created, and the operations are performed \n",
    "  on the newly created RDD. Thus, any lost data can be recovered easily and recreated. This feature makes Spark RDD \n",
    "  fault-tolerant.\n",
    "\n",
    "**5.Partitioning-**\n",
    "\n",
    "  Data items in RDDs are usually huge. This data is partitioned and send across different nodes for distributed computing.\n",
    "\n",
    "**6.Persistence-**\n",
    "\n",
    "  Intermediate results generated by RDD are stored to make the computation easy. It makes the process optimized.\n",
    "\n",
    "**7.Grained operation-**\n",
    "\n",
    "  Spark RDD offers two types of grained operations namely coarse-grained and fine-grained. The coarse-grained operation allows   us to transform the whole dataset while the fine-grained operation allows us to transform individual elements in the dataset.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc8d6b",
   "metadata": {},
   "source": [
    "### 3.Create spark RDD using Parallelized collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "392fcc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28c581ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8be4e4",
   "metadata": {},
   "source": [
    "### a. Create RDD with elements in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5c21a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Use_Case1\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7419fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_RDD = sc.parallelize([(\"Alice\",31,3),(\"Bob\",35,4),(\"Kanwar\",29,2),(\"Ravi\",28,1),(\"Dev\",37,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85901a",
   "metadata": {},
   "source": [
    "### b. Print all elements of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f28c92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 31, 3),\n",
       " ('Bob', 35, 4),\n",
       " ('Kanwar', 29, 2),\n",
       " ('Ravi', 28, 1),\n",
       " ('Dev', 37, 5)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffb41922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n",
      "|    _1| _2| _3|\n",
      "+------+---+---+\n",
      "| Alice| 31|  3|\n",
      "|   Bob| 35|  4|\n",
      "|Kanwar| 29|  2|\n",
      "|  Ravi| 28|  1|\n",
      "|   Dev| 37|  5|\n",
      "+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD to Dataframe\n",
    "MY_RDD = MY_RDD.toDF()\n",
    "MY_RDD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "025e0eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  Name|Age|Experience|\n",
      "+------+---+----------+\n",
      "| Alice| 31|         3|\n",
      "|   Bob| 35|         4|\n",
      "|Kanwar| 29|         2|\n",
      "|  Ravi| 28|         1|\n",
      "|   Dev| 37|         5|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the Column Names\n",
    "MY_RDD.withColumnRenamed(\"_1\",\"Name\").withColumnRenamed(\"_2\",\"Age\").withColumnRenamed(\"_3\",\"Experience\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12107aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(MY_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e24248",
   "metadata": {},
   "source": [
    "### c. Print count elements in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4625ad23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_RDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10b542",
   "metadata": {},
   "source": [
    "### d. Show use of foreach(f) operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f1cbb7",
   "metadata": {},
   "source": [
    "FOR EACH is an action operation in the spark that is available with DataFrame, RDD, and Datasets in pyspark \n",
    "to iterate over each and every element in the dataset. \n",
    "Foreach() applies f function to all the row of this dataframe.\n",
    "The forEach() method returns **undefined** and map() returns a new **rdd/dataframe** with the transformed elements. Even if they do the same job, the returning value remains different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d0976c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9c56fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(person):\n",
    "    print(person.Name)\n",
    "    return person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee424e4a",
   "metadata": {},
   "source": [
    "### e. What is the use of cache() and how to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dc657",
   "metadata": {},
   "source": [
    "**use:**\n",
    "you create a checkpoint in your spark application and if further down the execution of application any of the tasks fail \n",
    "your application will be able to recompute the lost RDD partition from the cache.\n",
    "\n",
    "to speed up applications that access the same RDD multiple times.\n",
    "\n",
    "There are two function calls for caching an RDD: cache() and persist(level: StorageLevel).\n",
    "\n",
    "The difference between cache() and persist() is that using **cache()** the default storage level is MEMORY_ONLY\n",
    "while using **persist()** we can use various storage levels (described below). \n",
    "\n",
    "**Advantages for Caching of DataFrame**\n",
    "\n",
    "Cost-efficient – Spark computations are very expensive hence reusing the computations are used to save cost.\n",
    "Time-efficient – Reusing repeated computations saves lots of time.\n",
    "Execution time – Saves execution time of the job and we can perform more jobs on the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbfff4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words got chached > True\n"
     ]
    }
   ],
   "source": [
    "MY_RDD.cache() \n",
    "caching = MY_RDD.persist().is_cached \n",
    "print (\"Words got chached > %s\" % (caching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f61890b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint, _3: bigint]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_RDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7aed5",
   "metadata": {},
   "source": [
    "### f. Show use of map(), reduce() and foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff51f2",
   "metadata": {},
   "source": [
    "The map method receives a function as a parameter then it applies on every element of Rdd/Dataframe and returns a new Rdd.\n",
    "It will give same no of records as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86772855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Row(_1='Alice', _2=31, _3=3), 1)\n",
      "(Row(_1='Bob', _2=35, _3=4), 1)\n",
      "(Row(_1='Kanwar', _2=29, _3=2), 1)\n",
      "(Row(_1='Ravi', _2=28, _3=1), 1)\n",
      "(Row(_1='Dev', _2=37, _3=5), 1)\n"
     ]
    }
   ],
   "source": [
    "# Use of Map\n",
    "MY_RDD1 = MY_RDD.rdd.map(lambda x: (x,1))\n",
    "for x in MY_RDD1.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4bdf8",
   "metadata": {},
   "source": [
    "#### Use Of Reduce\n",
    "Reduce()function is used to calculate min, max, and total of elements in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec4c1056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use of Reduce \n",
    "NEW_DATA = [10,9,8,7,6,5,4,3,2,1]\n",
    "RDD = sc.parallelize(NEW_DATA)\n",
    "RDD.reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f05174a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use of Reduce Second Method\n",
    "from operator import mul\n",
    "new_rdd = sc.parallelize([1,2,3,4,5,6])\n",
    "a = new_rdd.reduce(mul)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
