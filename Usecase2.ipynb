{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c14404",
   "metadata": {},
   "source": [
    " ##                                                                  Use Case 2 - Walmart Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2def0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78c6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a75106",
   "metadata": {},
   "source": [
    "### 1. Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bbd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b45c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkSession.builder.appName('walmart').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995f8c1",
   "metadata": {},
   "source": [
    "### 2. Load the Walmart Stock CSV File, have Spark infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378798b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"walmart_stock (1).csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74bfbb",
   "metadata": {},
   "source": [
    "### 3. What are the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08c2830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c14c5",
   "metadata": {},
   "source": [
    "### 4.What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496ec864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e3311",
   "metadata": {},
   "source": [
    "### 5. Print out the first 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5688ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2012-01-03', Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n",
       " Row(Date='2012-01-04', Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475),\n",
       " Row(Date='2012-01-05', Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539),\n",
       " Row(Date='2012-01-06', Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922),\n",
       " Row(Date='2012-01-09', Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc210868",
   "metadata": {},
   "source": [
    "### 6.Use describe() to learn about the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb059805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|      Date|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|      1258|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean|      null| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|      null|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|2012-01-03|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|2016-12-30|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57766cd5",
   "metadata": {},
   "source": [
    "### 7.In the above output there are many decimal points. No, you are asked \n",
    "to format the numbers to just show up to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e930155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+--------+---------+\n",
      "| Open| High|  Low|Close|  Volume|Adj Close|\n",
      "+-----+-----+-----+-----+--------+---------+\n",
      "|59.97|61.06|59.87|60.33|12668800|    52.62|\n",
      "|60.21|60.35|59.47|59.71| 9593300|    52.08|\n",
      "|59.35|59.62|58.37|59.42|12768200|    51.83|\n",
      "|59.42|59.45|58.87| 59.0| 8069400|    51.46|\n",
      "|59.03|59.55|58.92|59.18| 6679300|    51.62|\n",
      "|59.43|59.71|58.98|59.04| 6907300|    51.49|\n",
      "|59.06|59.53|59.04| 59.4| 6365600|    51.81|\n",
      "|59.79| 60.0| 59.4| 59.5| 7236400|     51.9|\n",
      "|59.18|59.61|59.01|59.54| 7729300|    51.93|\n",
      "|59.87|60.11|59.52|59.85| 8500000|     52.2|\n",
      "|59.79|60.03|59.65|60.01| 5911400|    52.34|\n",
      "|59.93|60.73|59.75|60.61| 9234600|    52.86|\n",
      "|60.75|61.25|60.67|61.01|10378800|    53.21|\n",
      "|60.81|60.98|60.51|60.91| 7134100|    53.13|\n",
      "|60.75| 62.0|60.75|61.39| 7362800|    53.54|\n",
      "|61.18|61.61|61.04|61.47| 5915800|    53.61|\n",
      "| 61.8|61.84|60.77|60.97| 7436200|    53.18|\n",
      "|60.86|61.12|60.54|60.71| 6287300|    52.95|\n",
      "|60.47|61.32|60.35| 61.3| 7636900|    53.47|\n",
      "|61.53|61.57|60.58|61.36| 9761500|    53.52|\n",
      "+-----+-----+-----+-----+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "df.select(round('Open',2).alias('Open') , round('High',2).alias('High'),round('Low',2).alias('Low'),\n",
    "round('Close',2).alias('Close'),round('Volume',2).alias('Volume'),round('Adj Close',2).alias('Adj Close')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391ee55",
   "metadata": {},
   "source": [
    "### 8.Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock \n",
    "traded for a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0463d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV Ratio|\n",
      "+--------------------+\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "|8.644477436914568E-6|\n",
      "|9.351828421515645E-6|\n",
      "| 8.29141562102703E-6|\n",
      "|7.712212102001476E-6|\n",
      "|7.071764823529412E-6|\n",
      "|1.015495466386981E-5|\n",
      "|6.576354146362592...|\n",
      "| 5.90145296180676E-6|\n",
      "|8.547679455011844E-6|\n",
      "|8.420709512685392E-6|\n",
      "|1.041448341728929...|\n",
      "|8.316075414862431E-6|\n",
      "|9.721183814992126E-6|\n",
      "|8.029436027707578E-6|\n",
      "|6.307432259386365E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hv = df.withColumn('HV Ratio', df['High']/df['Volume']).select(['HV Ratio'])\n",
    "df_hv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926eaca",
   "metadata": {},
   "source": [
    "### 9. What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac5625",
   "metadata": {},
   "source": [
    "orderBy() function is used to sort DataFrame by ascending or descending order based on single or multiple column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "963d0bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-01-13'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(df['High'].desc()).select(['Date']).head(1)[0]['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf141d9",
   "metadata": {},
   "source": [
    "### 10. What is the mean(Average) of the Close column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03313fbb",
   "metadata": {},
   "source": [
    "returns the average value from a particular column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d81f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean('Close')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41356665",
   "metadata": {},
   "source": [
    "### 11. What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdf8b4",
   "metadata": {},
   "source": [
    " to return the minimum and maximum value from a particular column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1bc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(Volume)|max(Volume)|\n",
      "+-----------+-----------+\n",
      "|    2094900|   80898100|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min , max\n",
    "df.select(min('Volume'),max('Volume')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6d3b1",
   "metadata": {},
   "source": [
    "### 12. How many days was the Close lower than 60 dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4acb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Close'] < 60).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a0468",
   "metadata": {},
   "source": [
    "### 13. What percentage of the time was the High greater than 80 dollars ? \n",
    "In other words, (Number of Days High>80)/(Total Days in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc2dcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.141494435612083"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter('High > 80').count() * 100/df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6da256",
   "metadata": {},
   "source": [
    "### 14. What is the Pearson correlation between High and Volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dd7708c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3384326061737161"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr('High','Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a42a3",
   "metadata": {},
   "source": [
    "### 15. What is the max High per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6ed5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|year(Date)|max(High)|\n",
      "+----------+---------+\n",
      "|      2012|77.599998|\n",
      "|      2013|81.370003|\n",
      "|      2014|88.089996|\n",
      "|      2015|90.970001|\n",
      "|      2016|75.190002|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "df.groupBy(year(\"Date\")).agg({\"High\":\"Max\"}).sort(\"year(Date)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a940f",
   "metadata": {},
   "source": [
    "### 16. What is the average Close for each Calendar Month? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2ec1f",
   "metadata": {},
   "source": [
    " to make a special signature for a column or table that is more often readable and shorter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a78058cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|month|       mean value|\n",
      "+-----+-----------------+\n",
      "|    1|71.44801958415842|\n",
      "|    2|  71.306804443299|\n",
      "|    3|71.77794377570092|\n",
      "|    4|72.97361900952382|\n",
      "|    5|72.30971688679247|\n",
      "|    6| 72.4953774245283|\n",
      "|    7|74.43971943925233|\n",
      "|    8|73.02981855454546|\n",
      "|    9|72.18411785294116|\n",
      "|   10|71.57854545454543|\n",
      "|   11| 72.1110893069307|\n",
      "|   12|72.84792478301885|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "meanv = df.groupBy(month(\"Date\").alias(\"month\")).agg(mean(\"Close\").alias(\"mean value\"))\n",
    "meanv.sort('month').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
